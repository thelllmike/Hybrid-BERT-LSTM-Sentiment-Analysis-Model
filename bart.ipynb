{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbadd35-277a-4d26-accc-9bbb06792c1f",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996639c9-9926-4233-81b5-689a8881dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.4-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.5 MB 243.2 kB/s eta 0:00:37\n",
      "   -- ------------------------------------- 0.5/9.5 MB 243.2 kB/s eta 0:00:37\n",
      "   -- ------------------------------------- 0.5/9.5 MB 243.2 kB/s eta 0:00:37\n",
      "   -- ------------------------------------- 0.5/9.5 MB 243.2 kB/s eta 0:00:37\n",
      "   --- ------------------------------------ 0.8/9.5 MB 282.0 kB/s eta 0:00:31\n",
      "   --- ------------------------------------ 0.8/9.5 MB 282.0 kB/s eta 0:00:31\n",
      "   --- ------------------------------------ 0.8/9.5 MB 282.0 kB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 1.0/9.5 MB 324.7 kB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 1.0/9.5 MB 324.7 kB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 1.3/9.5 MB 356.9 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 1.3/9.5 MB 356.9 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 1.3/9.5 MB 356.9 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 1.3/9.5 MB 356.9 kB/s eta 0:00:23\n",
      "   ------ --------------------------------- 1.6/9.5 MB 355.4 kB/s eta 0:00:23\n",
      "   ------ --------------------------------- 1.6/9.5 MB 355.4 kB/s eta 0:00:23\n",
      "   ------- -------------------------------- 1.8/9.5 MB 377.0 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 1.8/9.5 MB 377.0 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 1.8/9.5 MB 377.0 kB/s eta 0:00:21\n",
      "   -------- ------------------------------- 2.1/9.5 MB 386.3 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 2.1/9.5 MB 386.3 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 2.1/9.5 MB 386.3 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 2.1/9.5 MB 386.3 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 2.1/9.5 MB 386.3 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 2.1/9.5 MB 386.3 kB/s eta 0:00:20\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/9.5 MB 340.7 kB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 2.6/9.5 MB 293.8 kB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 2.6/9.5 MB 293.8 kB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 2.6/9.5 MB 293.8 kB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 2.6/9.5 MB 293.8 kB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 2.6/9.5 MB 293.8 kB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 2.6/9.5 MB 293.8 kB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 2.6/9.5 MB 293.8 kB/s eta 0:00:24\n",
      "   ------------ --------------------------- 2.9/9.5 MB 276.8 kB/s eta 0:00:24\n",
      "   ------------ --------------------------- 2.9/9.5 MB 276.8 kB/s eta 0:00:24\n",
      "   ------------ --------------------------- 2.9/9.5 MB 276.8 kB/s eta 0:00:24\n",
      "   ------------- -------------------------- 3.1/9.5 MB 284.8 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 3.1/9.5 MB 284.8 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 3.1/9.5 MB 284.8 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 3.1/9.5 MB 284.8 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 3.1/9.5 MB 284.8 kB/s eta 0:00:23\n",
      "   -------------- ------------------------- 3.4/9.5 MB 277.7 kB/s eta 0:00:22\n",
      "   -------------- ------------------------- 3.4/9.5 MB 277.7 kB/s eta 0:00:22\n",
      "   -------------- ------------------------- 3.4/9.5 MB 277.7 kB/s eta 0:00:22\n",
      "   -------------- ------------------------- 3.4/9.5 MB 277.7 kB/s eta 0:00:22\n",
      "   --------------- ------------------------ 3.7/9.5 MB 282.2 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 3.7/9.5 MB 282.2 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 3.7/9.5 MB 282.2 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 3.7/9.5 MB 282.2 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 3.7/9.5 MB 282.2 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 3.7/9.5 MB 282.2 kB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 273.1 kB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 273.1 kB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 273.1 kB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 273.1 kB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 273.1 kB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 273.1 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 268.6 kB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 268.6 kB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 268.6 kB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 268.6 kB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 268.6 kB/s eta 0:00:20\n",
      "   ------------------ --------------------- 4.5/9.5 MB 267.6 kB/s eta 0:00:19\n",
      "   ------------------ --------------------- 4.5/9.5 MB 267.6 kB/s eta 0:00:19\n",
      "   ------------------ --------------------- 4.5/9.5 MB 267.6 kB/s eta 0:00:19\n",
      "   ------------------ --------------------- 4.5/9.5 MB 267.6 kB/s eta 0:00:19\n",
      "   ------------------ --------------------- 4.5/9.5 MB 267.6 kB/s eta 0:00:19\n",
      "   ------------------ --------------------- 4.5/9.5 MB 267.6 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 4.7/9.5 MB 261.2 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 4.7/9.5 MB 261.2 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 4.7/9.5 MB 261.2 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 4.7/9.5 MB 261.2 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 4.7/9.5 MB 261.2 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 4.7/9.5 MB 261.2 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 4.7/9.5 MB 261.2 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 4.7/9.5 MB 261.2 kB/s eta 0:00:19\n",
      "   --------------------- ------------------ 5.0/9.5 MB 253.1 kB/s eta 0:00:18\n",
      "   --------------------- ------------------ 5.0/9.5 MB 253.1 kB/s eta 0:00:18\n",
      "   --------------------- ------------------ 5.0/9.5 MB 253.1 kB/s eta 0:00:18\n",
      "   --------------------- ------------------ 5.0/9.5 MB 253.1 kB/s eta 0:00:18\n",
      "   --------------------- ------------------ 5.0/9.5 MB 253.1 kB/s eta 0:00:18\n",
      "   --------------------- ------------------ 5.0/9.5 MB 253.1 kB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 5.2/9.5 MB 250.0 kB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 5.2/9.5 MB 250.0 kB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 5.2/9.5 MB 250.0 kB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 5.2/9.5 MB 250.0 kB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 5.2/9.5 MB 250.0 kB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 5.5/9.5 MB 250.0 kB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 5.5/9.5 MB 250.0 kB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 5.5/9.5 MB 250.0 kB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 5.5/9.5 MB 250.0 kB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 5.5/9.5 MB 250.0 kB/s eta 0:00:16\n",
      "   ------------------------ --------------- 5.8/9.5 MB 247.8 kB/s eta 0:00:15\n",
      "   ------------------------ --------------- 5.8/9.5 MB 247.8 kB/s eta 0:00:15\n",
      "   ------------------------ --------------- 5.8/9.5 MB 247.8 kB/s eta 0:00:15\n",
      "   ------------------------ --------------- 5.8/9.5 MB 247.8 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 6.0/9.5 MB 250.4 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 6.0/9.5 MB 250.4 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 6.0/9.5 MB 250.4 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 6.0/9.5 MB 250.4 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 6.0/9.5 MB 250.4 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 6.0/9.5 MB 250.4 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 6.0/9.5 MB 250.4 kB/s eta 0:00:14\n",
      "   -------------------------- ------------- 6.3/9.5 MB 247.4 kB/s eta 0:00:13\n",
      "   -------------------------- ------------- 6.3/9.5 MB 247.4 kB/s eta 0:00:13\n",
      "   -------------------------- ------------- 6.3/9.5 MB 247.4 kB/s eta 0:00:13\n",
      "   -------------------------- ------------- 6.3/9.5 MB 247.4 kB/s eta 0:00:13\n",
      "   -------------------------- ------------- 6.3/9.5 MB 247.4 kB/s eta 0:00:13\n",
      "   -------------------------- ------------- 6.3/9.5 MB 247.4 kB/s eta 0:00:13\n",
      "   -------------------------- ------------- 6.3/9.5 MB 247.4 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 6.6/9.5 MB 241.7 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 6.6/9.5 MB 241.7 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 6.6/9.5 MB 241.7 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 6.6/9.5 MB 241.7 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 6.6/9.5 MB 241.7 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 6.6/9.5 MB 241.7 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 6.6/9.5 MB 241.7 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 6.6/9.5 MB 241.7 kB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 237.2 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 237.2 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 237.2 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 237.2 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 237.2 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 237.2 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 237.2 kB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 7.1/9.5 MB 233.9 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 7.1/9.5 MB 233.9 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 7.1/9.5 MB 233.9 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 7.1/9.5 MB 233.9 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 7.1/9.5 MB 233.9 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 7.1/9.5 MB 233.9 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 7.1/9.5 MB 233.9 kB/s eta 0:00:11\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.3/9.5 MB 229.6 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 7.6/9.5 MB 184.2 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 7.9/9.5 MB 174.8 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 7.9/9.5 MB 174.8 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 7.9/9.5 MB 174.8 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 7.9/9.5 MB 174.8 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 7.9/9.5 MB 174.8 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 7.9/9.5 MB 174.8 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 7.9/9.5 MB 174.8 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 7.9/9.5 MB 174.8 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 168.4 kB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 168.4 kB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 168.4 kB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 168.4 kB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 168.4 kB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 8.4/9.5 MB 169.4 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 8.4/9.5 MB 169.4 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 8.4/9.5 MB 169.4 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 8.4/9.5 MB 169.4 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 8.4/9.5 MB 169.4 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 8.4/9.5 MB 169.4 kB/s eta 0:00:07\n",
      "   ------------------------------------ --- 8.7/9.5 MB 164.4 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 8.7/9.5 MB 164.4 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 8.7/9.5 MB 164.4 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 8.7/9.5 MB 164.4 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 8.7/9.5 MB 164.4 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 8.7/9.5 MB 164.4 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 8.7/9.5 MB 164.4 kB/s eta 0:00:05\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 8.9/9.5 MB 163.7 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.2/9.5 MB 143.2 kB/s eta 0:00:03\n",
      "   ---------------------------------------  9.4/9.5 MB 132.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 132.0 kB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Downloading safetensors-0.4.4-cp312-none-win_amd64.whl (286 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.5 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.44.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers torch pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10c1eac-65d2-437a-8354-6fbebc8eab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 1.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/1.8 MB 1.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/1.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.8 MB 1.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/1.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.8 MB 1.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.8 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/1.8 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.2/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.2/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 875.3 kB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1\n",
      "    Uninstalling pip-24.1:\n",
      "      Successfully uninstalled pip-24.1\n",
      "Successfully installed pip-24.2\n"
     ]
    }
   ],
   "source": [
    "#!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ac335-21ce-44bf-9901-4330f0e9e456",
   "metadata": {},
   "source": [
    " ## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0890675-0e77-4e2a-8493-5f389297fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bad3b59-d39d-438a-83fb-45f867f1075e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.3 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.3/2.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.3/2.3 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.3 MB 932.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e91bc8-e679-4441-b17e-38ed40bc5b15",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ef8f593-4c7b-487f-aad1-3d64682aca03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salon_name</th>\n",
       "      <th>beautician</th>\n",
       "      <th>Service_type</th>\n",
       "      <th>Review</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RAMI salon</td>\n",
       "      <td>Rami</td>\n",
       "      <td>haircut</td>\n",
       "      <td>Thank you somuch Slon Rami... you done it real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAMI salon</td>\n",
       "      <td>Madushanka</td>\n",
       "      <td>eyebrow shaping</td>\n",
       "      <td>I had my eyebrows shaped yesterday evening by ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAMI salon</td>\n",
       "      <td>Manoj</td>\n",
       "      <td>service</td>\n",
       "      <td>Unprofessional and bad service by manoj ðŸ˜”ðŸ¥ºcanâ€™...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAMI salon</td>\n",
       "      <td>Manoj</td>\n",
       "      <td>haircut</td>\n",
       "      <td>Highly recommended.This was my second time wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RAMI salon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>customer service</td>\n",
       "      <td>Bad bad serviceâ€¦. Very unprofessional Painful ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salon_name beautician        Service_type  \\\n",
       "0  RAMI salon        Rami            haircut   \n",
       "1  RAMI salon  Madushanka    eyebrow shaping   \n",
       "2  RAMI salon       Manoj            service   \n",
       "3  RAMI salon       Manoj            haircut   \n",
       "4  RAMI salon         NaN  customer service    \n",
       "\n",
       "                                              Review  Unnamed: 4  Unnamed: 5  \\\n",
       "0  Thank you somuch Slon Rami... you done it real...         NaN         NaN   \n",
       "1  I had my eyebrows shaped yesterday evening by ...         NaN         NaN   \n",
       "2  Unprofessional and bad service by manoj ðŸ˜”ðŸ¥ºcanâ€™...         NaN         NaN   \n",
       "3  Highly recommended.This was my second time wit...         NaN         NaN   \n",
       "4  Bad bad serviceâ€¦. Very unprofessional Painful ...         NaN         NaN   \n",
       "\n",
       "   Unnamed: 6 Unnamed: 7  \n",
       "0         NaN        NaN  \n",
       "1         NaN        NaN  \n",
       "2         NaN        NaN  \n",
       "3         NaN        NaN  \n",
       "4         NaN        NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_excel('RAMI salon.xlsx')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393ead8-e250-46c6-bdca-22642046c71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f247b18b-e893-4ff0-b2a8-6874c89a4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore specific FutureWarnings from transformers\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message=\".*`clean_up_tokenization_spaces`.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19f0f7-5084-468a-bb00-6ba6aaa2b64e",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d1c6c02-45ec-4157-8377-4af706b129bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 128])\n",
      "torch.Size([58, 128])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_excel('RAMI salon.xlsx')\n",
    "\n",
    "# Convert all entries in 'Review' to strings and handle NaN values\n",
    "data['Review'] = data['Review'].fillna('')  # Replace NaN with empty string\n",
    "data['Review'] = data['Review'].astype(str)  # Ensure all data is string\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_data(texts, max_length=128):\n",
    "    encoded_data = tokenizer(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Apply the tokenizer\n",
    "input_ids, attention_masks = tokenize_data(data['Review'].tolist())\n",
    "\n",
    "# Display shapes to verify\n",
    "print(input_ids.shape)\n",
    "print(attention_masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7fcd937-d78d-44b0-a4da-d81bb9d8593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy labels if actual labels are not available\n",
    "labels = torch.zeros(input_ids.size(0), dtype=torch.long)  # Example: All zeros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d11e0f-088d-4d29-8b63-d19de38ae18a",
   "metadata": {},
   "source": [
    "## Split Data into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2539390-0d1c-4fc7-b0d8-7b061251ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming labels are either correctly defined or dummy labels are created\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids, labels, random_state=2018, test_size=0.1\n",
    ")\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_masks, labels, random_state=2018, test_size=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d37ce-006b-4ad8-a0e9-ddb8e9fb0be1",
   "metadata": {},
   "source": [
    "## Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7eb33866-8de4-4c82-954a-d0bb38cfb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19a8c5c5-42e2-4f0d-9f30-9c3c14c6760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ee41684-eee6-4bc8-b29e-55712cf02984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.18.0)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.4.0-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchaudio-2.3.1-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.3.0-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchaudio-2.3.0-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.4 MB 143.4 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 0.8/2.4 MB 119.0 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 0.8/2.4 MB 119.0 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 0.8/2.4 MB 119.0 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 0.8/2.4 MB 119.0 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 0.8/2.4 MB 119.0 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 0.8/2.4 MB 119.0 kB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 136.8 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 136.8 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 136.8 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 136.8 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 136.8 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 136.8 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 148.1 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 1.6/2.4 MB 138.2 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 1.6/2.4 MB 138.2 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 1.6/2.4 MB 138.2 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 1.6/2.4 MB 138.2 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 1.8/2.4 MB 152.7 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.8/2.4 MB 152.7 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.8/2.4 MB 152.7 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.8/2.4 MB 152.7 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.8/2.4 MB 152.7 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.8/2.4 MB 152.7 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 160.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 160.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 160.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 160.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 160.7 kB/s eta 0:00:02\n",
      "   ---------------------------------------- 2.4/2.4 MB 167.6 kB/s eta 0:00:00\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eee30b11-3144-4392-8a49-37d7a6cc5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"parameter name that contains `beta` will be renamed internally to `bias`\")\n",
    "warnings.filterwarnings('ignore', message=\"parameter name that contains `gamma` will be renamed internally to `weight`\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169f8fe-618b-4650-a7f6-fed0e4e57360",
   "metadata": {},
   "source": [
    " ## Load BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fda74493-efff-4dc1-b32b-1d9f09317200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,  # Adjust based on your task\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "model.to(\"cpu\")  # Explicitly move the model to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910bae9-41b7-4ce1-bc16-c3b5630de010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e634f2d7-76f4-459e-a704-137d93200522",
   "metadata": {},
   "source": [
    "## Set Up Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccb15616-8ff9-49a9-a06d-f774acac1dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micheal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e16d30-f9dc-4e95-aa44-3f6ebb9a4399",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "413cda5a-d812-41fe-8938-38af7a2a66af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.47\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.35\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.34\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.29\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.29\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.25\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.24\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.23\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming model and tokenizer are loaded\n",
    "# Assuming train_dataloader and validation_dataloader are set up\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Parameters\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Training loop\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7a67c-4d2f-40ea-a96a-93ad6b0c8d26",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98a5626e-a7cb-401b-b1aa-16b7d4d0c301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('path_to_save_tokenizer\\\\tokenizer_config.json',\n",
       " 'path_to_save_tokenizer\\\\special_tokens_map.json',\n",
       " 'path_to_save_tokenizer\\\\vocab.txt',\n",
       " 'path_to_save_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model after training\n",
    "model.save_pretrained('path_to_save_model')\n",
    "tokenizer.save_pretrained('path_to_save_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e60bac74-6546-43cc-845b-38e4523c7532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended based on positive reviews:\n"
     ]
    }
   ],
   "source": [
    "# Example to filter positive reviews (assuming class '1' is positive)\n",
    "recommended_reviews = [test_reviews[i] for i in range(len(test_reviews)) if test_predictions[i] == 1]\n",
    "\n",
    "print(\"Recommended based on positive reviews:\")\n",
    "for review in recommended_reviews:\n",
    "    print(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b104bf69-4288-4f94-8956-9aa00a97cd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Beautician(s):\n",
      "    name  score\n",
      "2  Manoj      3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example beautician data\n",
    "beauticians = pd.DataFrame({\n",
    "    'name': ['Rami', 'Madushanka', 'Manoj', 'Sasha'],\n",
    "    'style': ['Modern', 'Traditional', 'Natural', 'Modern'],\n",
    "    'interaction': ['Conversational', 'Quiet', 'Informative', 'Supportive'],\n",
    "    'speed': ['Quick', 'Thorough', 'Quick', 'Thorough'],\n",
    "    'personality': ['Professional', 'Friendly', 'Cheerful', 'Disciplined'],\n",
    "    'average_time': ['30 min', '45 min', '30 min', '1h']\n",
    "})\n",
    "\n",
    "# User preferences\n",
    "user_preferences = {\n",
    "    'style': 'Natural',\n",
    "    'interaction': 'Informative',\n",
    "    'speed': 'Quick',\n",
    "    'personality': 'Professional',\n",
    "    'average_time': '45 min'\n",
    "}\n",
    "\n",
    "# Scoring function\n",
    "def score_beautician(beautician, preferences):\n",
    "    score = 0\n",
    "    for key, value in preferences.items():\n",
    "        if beautician[key] == value:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "# Apply scoring\n",
    "beauticians['score'] = beauticians.apply(score_beautician, axis=1, preferences=user_preferences)\n",
    "\n",
    "# Filter for highest score\n",
    "recommended_beauticians = beauticians[beauticians['score'] == beauticians['score'].max()]\n",
    "\n",
    "print(\"Recommended Beautician(s):\")\n",
    "print(recommended_beauticians[['name', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2af645e-53e2-4dea-ad4d-5aa07c7308ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Beautician(s) based on Positive Reviews and Preferences:\n",
      "   name                                             review  score\n",
      "0  Rami  Rami provided an excellent service with a mode...      6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample beautician data\n",
    "beauticians = pd.DataFrame({\n",
    "    'name': ['Rami', 'Madushanka', 'Manoj', 'Sasha'],\n",
    "    'style': ['Modern', 'Traditional', 'Natural', 'Modern'],\n",
    "    'interaction': ['Conversational', 'Quiet', 'Informative', 'Supportive'],\n",
    "    'speed': ['Quick', 'Thorough', 'Quick', 'Thorough'],\n",
    "    'personality': ['Professional', 'Friendly', 'Cheerful', 'Disciplined'],\n",
    "    'average_time': ['30 min', '45 min', '30 min', '1h'],\n",
    "    'review': [\n",
    "        \"Rami provided an excellent service with a modern haircut that was quick and professional. Highly recommended!\",\n",
    "        \"Madushanka's work on traditional makeup wasn't up to par this time, lacked the usual charm.\",\n",
    "        \"Manoj was quite informative and friendly while providing a quick and efficient haircut. Very happy with the results!\",\n",
    "        \"Sashaâ€™s work was disciplined but too slow, and the support was lacking during the long session.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# User preferences\n",
    "user_preferences = {\n",
    "    'style': 'Modern',\n",
    "    'interaction': 'Conversational',\n",
    "    'speed': 'Quick',\n",
    "    'personality': 'Professional',\n",
    "    'average_time': '30 min'\n",
    "}\n",
    "\n",
    "# Function to score each beautician based on user preferences and sentiment analysis\n",
    "def enhanced_score_beautician(beautician, preferences):\n",
    "    score = 0\n",
    "    # Attribute matching\n",
    "    for key in ['style', 'interaction', 'speed', 'personality', 'average_time']:\n",
    "        if beautician[key] == preferences[key]:\n",
    "            score += 1\n",
    "    \n",
    "    # Sentiment analysis on the review\n",
    "    sentiment = TextBlob(beautician['review']).sentiment.polarity\n",
    "    if sentiment > 0:  # Positive sentiment boosts the score\n",
    "        score += 1\n",
    "\n",
    "    return score\n",
    "\n",
    "# Apply the scoring function\n",
    "beauticians['score'] = beauticians.apply(enhanced_score_beautician, axis=1, preferences=user_preferences)\n",
    "\n",
    "# Recommend beauticians based on the highest score\n",
    "recommended_beauticians = beauticians[beauticians['score'] == beauticians['score'].max()]\n",
    "print(\"Recommended Beautician(s) based on Positive Reviews and Preferences:\")\n",
    "print(recommended_beauticians[['name', 'review', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a78cd712-1643-40a5-be1b-b2dcdec487d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting nltk>=3.8 (from textblob)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\micheal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\micheal\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/626.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/626.3 kB ? eta -:--:--\n",
      "   -------------------------------------- 626.3/626.3 kB 630.1 kB/s eta 0:00:00\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 390.1 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.5/1.5 MB 390.1 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 508.5 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 592.2 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 592.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 633.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 639.3 kB/s eta 0:00:00\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.6.7\n",
      "    Uninstalling nltk-3.6.7:\n",
      "      Successfully uninstalled nltk-3.6.7\n",
      "Successfully installed nltk-3.8.1 textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "label-studio-converter 0.0.58 requires nltk==3.6.7, but you have nltk 3.8.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48e8fefb-584f-4f10-aa8d-bd9435caf75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Micheal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7b624e4-91a8-4456-94e0-7650ec1d4224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Beautician(s) based on Positive Reviews and Preferences:\n",
      "   name                                             review  score\n",
      "0  Rami  Rami provided an excellent service with a mode...      6\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = 'path_to_save_model'\n",
    "tokenizer_path = 'path_to_save_tokenizer'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Ensure the model and tokenizer are correctly loaded\n",
    "# Example beautician data\n",
    "beauticians = pd.DataFrame({\n",
    "    'name': ['Rami', 'Madushanka', 'Manoj', 'Sasha'],\n",
    "    'style': ['Modern', 'Traditional', 'Natural', 'Modern'],\n",
    "    'interaction': ['Conversational', 'Quiet', 'Informative', 'Supportive'],\n",
    "    'speed': ['Quick', 'Thorough', 'Quick', 'Thorough'],\n",
    "    'personality': ['Professional', 'Friendly', 'Cheerful', 'Disciplined'],\n",
    "    'average_time': ['30 min', '45 min', '30 min', '1h'],\n",
    "    'review': [\n",
    "        \"Rami provided an excellent service with a modern haircut that was quick and professional. Highly recommended!\",\n",
    "        \"Madushanka's work on traditional makeup wasn't up to par this time, lacked the usual charm.\",\n",
    "        \"Manoj was quite informative and friendly while providing a quick and efficient haircut. Very happy with the results!\",\n",
    "        \"Sashaâ€™s work was disciplined but too slow, and the support was lacking during the long session.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# User preferences\n",
    "user_preferences = {\n",
    "    'style': 'Modern',\n",
    "    'interaction': 'Conversational',\n",
    "    'speed': 'Quick',\n",
    "    'personality': 'Professional',\n",
    "    'average_time': '30 min'\n",
    "}\n",
    "\n",
    "# Function to score each beautician based on user preferences and sentiment analysis\n",
    "def enhanced_score_beautician(beautician, preferences):\n",
    "    score = 0\n",
    "    # Attribute matching\n",
    "    for key in ['style', 'interaction', 'speed', 'personality', 'average_time']:\n",
    "        if beautician[key] == preferences[key]:\n",
    "            score += 1\n",
    "    \n",
    "    # Sentiment analysis on the review\n",
    "    sentiment = TextBlob(beautician['review']).sentiment.polarity\n",
    "    if sentiment > 0:  # Positive sentiment boosts the score\n",
    "        score += 1\n",
    "\n",
    "    return score\n",
    "\n",
    "# Apply the scoring function\n",
    "beauticians['score'] = beauticians.apply(enhanced_score_beautician, axis=1, preferences=user_preferences)\n",
    "\n",
    "# Recommend beauticians based on the highest score\n",
    "recommended_beauticians = beauticians[beauticians['score'] == beauticians['score'].max()]\n",
    "print(\"Recommended Beautician(s) based on Positive Reviews and Preferences:\")\n",
    "print(recommended_beauticians[['name', 'review', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "398b5792-52d4-4047-8985-edf0fb31da6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Beautician(s) based on Reviews and User Preferences:\n",
      "         name  final_score\n",
      "0        Rami     5.212096\n",
      "2       Manoj     3.064991\n",
      "3       Sasha     2.462529\n",
      "1  Madushanka     2.396789\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Sample data and preferences\n",
    "beauticians = pd.DataFrame({\n",
    "    'name': ['Rami', 'Madushanka', 'Manoj', 'Sasha'],\n",
    "    'review': [\n",
    "        \"Rami provided an excellent service with a modern haircut that was quick and professional. Highly recommended!\",\n",
    "        \"Madushanka's work on traditional makeup wasn't up to par this time, lacked the usual charm.\",\n",
    "        \"Manoj was quite informative and friendly while providing a quick and efficient haircut. Very happy with the results!\",\n",
    "        \"Sashaâ€™s work was disciplined but too slow, and the support was lacking during the long session.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Preferences with keywords (example)\n",
    "preference_keywords = {\n",
    "    'style': ['modern'],\n",
    "    'interaction': ['conversational'],\n",
    "    'speed': ['quick'],\n",
    "    'personality': ['professional'],\n",
    "    'average_time': ['30 min']\n",
    "}\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = 'path_to_save_model'\n",
    "tokenizer_path = 'path_to_save_tokenizer'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to predict sentiment from reviews\n",
    "def predict_sentiment(reviews):\n",
    "    encoded_reviews = tokenizer(reviews, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = encoded_reviews['input_ids'].to(device)\n",
    "    attention_mask = encoded_reviews['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    scores = torch.softmax(logits, dim=1)[:, 1]  # Assuming class 1 is positive sentiment\n",
    "    return scores.cpu().numpy()\n",
    "\n",
    "# Predict sentiments\n",
    "beauticians['sentiment_score'] = predict_sentiment(beauticians['review'].tolist())\n",
    "\n",
    "# Score beauticians based on user preferences and sentiment\n",
    "def final_score(row, keywords):\n",
    "    score = row['sentiment_score'] * 10  # Scale sentiment score\n",
    "    review_lower = row['review'].lower()\n",
    "    for key, words in keywords.items():\n",
    "        if any(word in review_lower for word in words):\n",
    "            score += 1  # Increment score for each matched keyword\n",
    "    return score\n",
    "\n",
    "beauticians['final_score'] = beauticians.apply(final_score, axis=1, keywords=preference_keywords)\n",
    "\n",
    "# Select top recommended beauticians\n",
    "recommended_beauticians = beauticians.sort_values(by='final_score', ascending=False)\n",
    "print(\"Recommended Beautician(s) based on Reviews and User Preferences:\")\n",
    "print(recommended_beauticians[['name', 'final_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61219a51-18b5-45b8-8d59-281d2f7d06d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
