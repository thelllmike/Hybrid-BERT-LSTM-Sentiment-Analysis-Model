{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "515da1fe-be05-4f99-bd04-2fd1c45a9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d408f77-4874-4349-9c27-3ca55222e24a",
   "metadata": {},
   "source": [
    "# Load your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d2d07f5-8aff-4db8-868c-1f0d4335c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_excel('RAMI salon.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8edcf3-8245-4db1-a908-4258d3728045",
   "metadata": {},
   "source": [
    "# Assuming the dataset contains 'Text', 'Emoji', and 'Label' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456c794-7125-4ee3-a965-dc389ddc2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify according to the actual column names in 'RAMI salon.xlsx'\n",
    "texts = data['Text'].tolist()  # Column for BERT text input\n",
    "emojis = data['Emoji'].tolist()  # Column for emoji indices (for LSTM)\n",
    "labels = data['Label'].tolist()  # Column for target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc978a9-7eee-4724-a69c-848de1418999",
   "metadata": {},
   "source": [
    "# Prepare text and emoji inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9daa3d6e-6da0-45e6-850c-cc3fe85f78ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ninduki\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d297e2-153f-4bcc-b895-dd0d85c5c75b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rename based on the actual column name you found in the previous step\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview \u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Rename based on the actual column name you found in the previous step\n",
    "data.rename(columns={'Review ': 'Review'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8072cd3-72fa-409d-b29f-bb2346a9c34b",
   "metadata": {},
   "source": [
    "# Tokenize the text for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb1df55-2b8b-4cfd-a79a-d3c0dc545a47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Replace 'Review' with the correct column name (e.g., 'Sentiment')\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenizer\u001b[38;5;241m.\u001b[39mencode(x, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace 'Review' with the correct column name (e.g., 'Sentiment')\n",
    "data['input_ids'] = data['Sentiment'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=64, truncation=True, padding='max_length'))\n",
    "data['attention_mask'] = data['input_ids'].apply(lambda x: [1 if i > 0 else 0 for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464ab8cf-8f35-4e5f-9dbb-63ac31a587ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Inspect the first few rows of the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Inspect the first few rows of the dataset\n",
    "print(data.columns)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2e5685d-8e6c-43d1-87e6-eb4cb2fbcc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Emoji', 'Sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = 'Emoji_Sentiment_Data_200.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Print the column names\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a5925cb-a4e7-47f4-a92d-6d1f95f66014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emoji Sentiment  Emoji_index\n",
      "0     ðŸ˜€  positive           92\n",
      "1     ðŸ˜ƒ  positive           95\n",
      "2     ðŸ˜„  positive           96\n",
      "3     ðŸ˜  positive           93\n",
      "4     ðŸ˜†  positive           98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the 'Emoji' column\n",
    "emoji_encoder = LabelEncoder()\n",
    "data['Emoji_index'] = emoji_encoder.fit_transform(data['Emoji'])\n",
    "\n",
    "# Verify that 'Emoji_index' has been added\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b4245-e5e1-4946-bcb1-a8a88cfa4bd4",
   "metadata": {},
   "source": [
    "# Encode emojis to indices (if emojis are categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c8c4974-77e5-46d9-b112-3d17ddf872c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names to remove any potential trailing spaces\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Now fit the LabelEncoder\n",
    "emoji_encoder = LabelEncoder()\n",
    "data['Emoji_index'] = emoji_encoder.fit_transform(data['Emoji'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb4fc7-2507-4024-883f-99749ffb14a8",
   "metadata": {},
   "source": [
    "# Encode sentiments to labels (assuming classification task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9831b13-6cf4-4e94-ba9a-6a4922ccc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'Sentiment' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment_label'] = label_encoder.fit_transform(data['Sentiment'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1cdb8f-5977-4b76-8b7c-bd615f268cb9",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8cfce3ef-97dd-4260-8f76-f98dd86c3ace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micheal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the 'Sentiment' column (or correct column if different)\n",
    "data['input_ids'] = data['Sentiment'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=64, truncation=True, padding='max_length'))\n",
    "data['attention_mask'] = data['input_ids'].apply(lambda x: [1 if i > 0 else 0 for i in x])\n",
    "\n",
    "# Encode the 'Sentiment' column into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment_encoded'] = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Extract data for model training\n",
    "X = data['Emoji_index'].values\n",
    "y = data['Sentiment_encoded'].values\n",
    "\n",
    "# Convert numerical sentiment labels to one-hot encoding\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Now y will contain one-hot encoded labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe6165-3a19-452a-b64f-455a41e6b42e",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e89354a-a18f-45d7-842c-ce6201c882f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_input_ids shape: (236, 64)\n",
      "X_attention_masks shape: (236, 64)\n",
      "X_emoji_indices shape: (236,)\n",
      "y shape: (236,)\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows with missing data in the relevant columns\n",
    "# Ensure input_ids, attention_mask, Emoji_index, and Sentiment_encoded are aligned\n",
    "data_cleaned = data.dropna(subset=['input_ids', 'attention_mask', 'Emoji_index', 'Sentiment_encoded'])\n",
    "\n",
    "# Recreate the arrays after cleaning to ensure consistent lengths\n",
    "X_input_ids = np.array(data_cleaned['input_ids'].tolist())\n",
    "X_attention_masks = np.array(data_cleaned['attention_mask'].tolist())\n",
    "X_emoji_indices = np.array(data_cleaned['Emoji_index'].values)\n",
    "y = np.array(data_cleaned['Sentiment_encoded'].values)\n",
    "\n",
    "# Now ensure consistent lengths\n",
    "print(f\"X_input_ids shape: {X_input_ids.shape}\")\n",
    "print(f\"X_attention_masks shape: {X_attention_masks.shape}\")\n",
    "print(f\"X_emoji_indices shape: {X_emoji_indices.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Ensure all arrays have the same number of rows\n",
    "if len(X_input_ids) == len(X_attention_masks) == len(X_emoji_indices) == len(y):\n",
    "    # Proceed with train-test split\n",
    "    X_train_ids, X_val_ids, X_train_masks, X_val_masks, X_train_emojis, X_val_emojis, y_train, y_val = train_test_split(\n",
    "        X_input_ids, X_attention_masks, X_emoji_indices, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert to TensorDatasets\n",
    "    train_data = TensorDataset(torch.tensor(X_train_ids), torch.tensor(X_train_masks), torch.tensor(X_train_emojis), torch.tensor(y_train))\n",
    "    val_data = TensorDataset(torch.tensor(X_val_ids), torch.tensor(X_val_masks), torch.tensor(X_val_emojis), torch.tensor(y_val))\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=32)\n",
    "    validation_dataloader = DataLoader(val_data, sampler=RandomSampler(val_data), batch_size=32)\n",
    "else:\n",
    "    print(\"Error: Arrays still have inconsistent lengths after cleaning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0b112-0295-497d-be25-141f1b4b290b",
   "metadata": {},
   "source": [
    "# Define the combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6d6f61-cc94-42a7-b366-9f456daad008",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBERTLSTMModel\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, bert_model_name, max_emojis, embedding_size, lstm_hidden_size, num_labels):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(BERTLSTMModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BERTLSTMModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, max_emojis, embedding_size, lstm_hidden_size, num_labels):\n",
    "        super(BERTLSTMModel, self).__init__()\n",
    "        \n",
    "        # BERT model for sequence classification\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Emoji Embedding and LSTM model\n",
    "        self.emoji_embedding = nn.Embedding(num_embeddings=max_emojis, embedding_dim=embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + lstm_hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, emoji_indices):\n",
    "        # Get BERT embeddings\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled_output = bert_outputs.pooler_output\n",
    "        bert_pooled_output = self.dropout(bert_pooled_output)\n",
    "        \n",
    "        # Ensure emoji_indices is 2D (batch_size, sequence_length)\n",
    "        if emoji_indices.dim() == 1:\n",
    "            emoji_indices = emoji_indices.unsqueeze(1)  # Add a sequence length dimension if necessary\n",
    "        \n",
    "        # Get emoji embeddings and pass through LSTM\n",
    "        emoji_embeds = self.emoji_embedding(emoji_indices)\n",
    "        print(f\"emoji_embeds shape: {emoji_embeds.shape}\")  # Debugging shape of emoji embeddings\n",
    "        \n",
    "        lstm_out, _ = self.lstm(emoji_embeds)\n",
    "        print(f\"lstm_out shape: {lstm_out.shape}\")  # Debugging shape of LSTM output\n",
    "        \n",
    "        # Handle both 2D and 3D outputs\n",
    "        if lstm_out.dim() == 3:\n",
    "            lstm_out = lstm_out[:, -1, :]  # Take the last output of the LSTM (3D case)\n",
    "        elif lstm_out.dim() == 2:\n",
    "            # In case of 2D output, handle directly\n",
    "            pass  # lstm_out is already 2D, no need for further indexing\n",
    "        \n",
    "        # Concatenate BERT and LSTM outputs\n",
    "        combined_output = torch.cat((bert_pooled_output, lstm_out), dim=1)\n",
    "        \n",
    "        # Pass through the classifier\n",
    "        logits = self.classifier(combined_output)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd654fb2-9524-4fae-8340-c83ad26529c2",
   "metadata": {},
   "source": [
    "# Optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6651b348-5700-413b-a97c-3c1fdef2fa94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdamW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdamW\u001b[49m(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      3\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m epochs\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AdamW' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04194c47-3d0f-4b05-9b16-12389079683c",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d1c870-d904-41ee-b3e1-8bfb463df9de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[43mepochs\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m======== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_i\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ========\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_emoji_indices = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, emoji_indices=b_emoji_indices)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(outputs, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    # Validation\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_emoji_indices = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask, emoji_indices=b_emoji_indices)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(outputs, b_labels)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        labels = b_labels.cpu().numpy()\n",
    "\n",
    "        total_eval_accuracy += np.sum(preds == labels) / len(labels)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(f\"  Accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f8a418d-dc07-497b-9c86-4301dbc5f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sachi\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 117\u001b[0m\n\u001b[0;32m    113\u001b[0m b_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    115\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 117\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memoji_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_emoji_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(outputs, b_labels)\n\u001b[0;32m    119\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 79\u001b[0m, in \u001b[0;36mBERTLSTMModel.forward\u001b[1;34m(self, input_ids, attention_mask, emoji_indices)\u001b[0m\n\u001b[0;32m     77\u001b[0m emoji_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memoji_embedding(emoji_indices)\n\u001b[0;32m     78\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(emoji_embeds)\n\u001b[1;32m---> 79\u001b[0m lstm_out \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Take the last LSTM output\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Concatenate BERT and LSTM outputs\u001b[39;00m\n\u001b[0;32m     82\u001b[0m combined_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((bert_pooled_output, lstm_out), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load your dataset (update the file path as necessary)\n",
    "data_path = 'Emoji_Sentiment_Data_200.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Clean column names to remove trailing spaces\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Encode 'Emoji' and 'Sentiment' columns\n",
    "emoji_encoder = LabelEncoder()\n",
    "data['Emoji_index'] = emoji_encoder.fit_transform(data['Emoji'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment_encoded'] = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the 'Sentiment' column for BERT\n",
    "data['input_ids'] = data['Sentiment'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=64, truncation=True, padding='max_length'))\n",
    "data['attention_mask'] = data['input_ids'].apply(lambda x: [1 if i > 0 else 0 for i in x])\n",
    "\n",
    "# Drop rows with missing data\n",
    "data_cleaned = data.dropna(subset=['input_ids', 'attention_mask', 'Emoji_index', 'Sentiment_encoded'])\n",
    "\n",
    "# Convert columns to numpy arrays\n",
    "X_input_ids = np.array(data_cleaned['input_ids'].tolist())\n",
    "X_attention_masks = np.array(data_cleaned['attention_mask'].tolist())\n",
    "X_emoji_indices = np.array(data_cleaned['Emoji_index'].values)\n",
    "y = to_categorical(data_cleaned['Sentiment_encoded'].values)\n",
    "\n",
    "# Train-test split\n",
    "X_train_ids, X_val_ids, X_train_masks, X_val_masks, X_train_emojis, X_val_emojis, y_train, y_val = train_test_split(\n",
    "    X_input_ids, X_attention_masks, X_emoji_indices, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to TensorDatasets\n",
    "train_data = TensorDataset(torch.tensor(X_train_ids), torch.tensor(X_train_masks), torch.tensor(X_train_emojis), torch.tensor(y_train))\n",
    "val_data = TensorDataset(torch.tensor(X_val_ids), torch.tensor(X_val_masks), torch.tensor(X_val_emojis), torch.tensor(y_val))\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=32)\n",
    "validation_dataloader = DataLoader(val_data, sampler=RandomSampler(val_data), batch_size=32)\n",
    "\n",
    "# Define the BERT-LSTM model\n",
    "class BERTLSTMModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, max_emojis, embedding_size, lstm_hidden_size, num_labels):\n",
    "        super(BERTLSTMModel, self).__init__()\n",
    "        \n",
    "        # BERT model for sequence classification\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Emoji Embedding and LSTM model\n",
    "        self.emoji_embedding = nn.Embedding(num_embeddings=max_emojis, embedding_dim=embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + lstm_hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, emoji_indices):\n",
    "        # BERT embeddings\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled_output = bert_outputs.pooler_output\n",
    "        bert_pooled_output = self.dropout(bert_pooled_output)\n",
    "        \n",
    "        # Emoji LSTM embeddings\n",
    "        emoji_embeds = self.emoji_embedding(emoji_indices)\n",
    "        lstm_out, _ = self.lstm(emoji_embeds)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the last LSTM output\n",
    "\n",
    "        # Concatenate BERT and LSTM outputs\n",
    "        combined_output = torch.cat((bert_pooled_output, lstm_out), dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(combined_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Instantiate the model\n",
    "model = BERTLSTMModel('bert-base-uncased', max_emojis=len(emoji_encoder.classes_), embedding_size=50, lstm_hidden_size=50, num_labels=y.shape[1])\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_emoji_indices = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, emoji_indices=b_emoji_indices)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_emoji_indices = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask, emoji_indices=b_emoji_indices)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(outputs, b_labels)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        labels = b_labels.cpu().numpy()\n",
    "        total_eval_accuracy += np.sum(preds == labels) / len(labels)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(f\"  Accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef86f0b5-71d1-4cb3-a0dd-b33d7b8d5e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012bd204-0581-466c-b5f7-cbf52a85185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (update the file path as necessary)\n",
    "data_path = 'Emoji_Sentiment_Data_200.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Clean column names to remove trailing spaces\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Encode 'Emoji' and 'Sentiment' columns\n",
    "emoji_encoder = LabelEncoder()\n",
    "data['Emoji_index'] = emoji_encoder.fit_transform(data['Emoji'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment_encoded'] = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7f8bd8-8629-4680-ab35-c970bc8a9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the 'Sentiment' column for BERT\n",
    "data['input_ids'] = data['Sentiment'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=64, truncation=True, padding='max_length'))\n",
    "data['attention_mask'] = data['input_ids'].apply(lambda x: [1 if i > 0 else 0 for i in x])\n",
    "\n",
    "# Drop rows with missing data\n",
    "data_cleaned = data.dropna(subset=['input_ids', 'attention_mask', 'Emoji_index', 'Sentiment_encoded'])\n",
    "\n",
    "# Convert columns to numpy arrays\n",
    "X_input_ids = np.array(data_cleaned['input_ids'].tolist())\n",
    "X_attention_masks = np.array(data_cleaned['attention_mask'].tolist())\n",
    "X_emoji_indices = np.array(data_cleaned['Emoji_index'].values)\n",
    "y = to_categorical(data_cleaned['Sentiment_encoded'].values)\n",
    "\n",
    "# Train-test split\n",
    "X_train_ids, X_val_ids, X_train_masks, X_val_masks, X_train_emojis, X_val_emojis, y_train, y_val = train_test_split(\n",
    "    X_input_ids, X_attention_masks, X_emoji_indices, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to TensorDatasets\n",
    "train_data = TensorDataset(torch.tensor(X_train_ids), torch.tensor(X_train_masks), torch.tensor(X_train_emojis), torch.tensor(y_train))\n",
    "val_data = TensorDataset(torch.tensor(X_val_ids), torch.tensor(X_val_masks), torch.tensor(X_val_emojis), torch.tensor(y_val))\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=32)\n",
    "validation_dataloader = DataLoader(val_data, sampler=RandomSampler(val_data), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e2ebaf-d0fb-4105-af0b-651f1ae3376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BERT-LSTM model\n",
    "class BERTLSTMModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, max_emojis, embedding_size, lstm_hidden_size, num_labels):\n",
    "        super(BERTLSTMModel, self).__init__()\n",
    "        \n",
    "        # BERT model for sequence classification\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Emoji Embedding and LSTM model\n",
    "        self.emoji_embedding = nn.Embedding(num_embeddings=max_emojis, embedding_dim=embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + lstm_hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, emoji_indices):\n",
    "        # BERT embeddings\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled_output = bert_outputs.pooler_output\n",
    "        bert_pooled_output = self.dropout(bert_pooled_output)\n",
    "        \n",
    "        # Ensure emoji_indices is 2D (batch_size, sequence_length)\n",
    "        if emoji_indices.dim() == 1:\n",
    "            emoji_indices = emoji_indices.unsqueeze(1)  # Add a sequence length dimension if necessary\n",
    "\n",
    "        # Emoji LSTM embeddings\n",
    "        emoji_embeds = self.emoji_embedding(emoji_indices)\n",
    "        \n",
    "        # Check the shape of the emoji embeddings\n",
    "        print(f\"emoji_embeds shape: {emoji_embeds.shape}\")  # Debugging output\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(emoji_embeds)\n",
    "        \n",
    "        # If LSTM output is 3D, take the last LSTM output; otherwise, handle 2D output\n",
    "        if lstm_out.dim() == 3:\n",
    "            lstm_out = lstm_out[:, -1, :]  # Take the last output in the sequence (3D case)\n",
    "        elif lstm_out.dim() == 2:\n",
    "            pass  # LSTM output is already 2D, no further indexing needed\n",
    "\n",
    "        # Concatenate BERT and LSTM outputs\n",
    "        combined_output = torch.cat((bert_pooled_output, lstm_out), dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(combined_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf173a6-6529-44cf-8347-b0817a446884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 1.05\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.80\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.58\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.39\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.30\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.22\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.17\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.12\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.10\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([28, 1, 50])\n",
      "  Average training loss: 0.09\n",
      "Running Validation...\n",
      "emoji_embeds shape: torch.Size([32, 1, 50])\n",
      "emoji_embeds shape: torch.Size([16, 1, 50])\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.05\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = BERTLSTMModel('bert-base-uncased', max_emojis=len(emoji_encoder.classes_), embedding_size=50, lstm_hidden_size=50, num_labels=y.shape[1])\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_emoji_indices = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, emoji_indices=b_emoji_indices)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    # Validation loop\n",
    "print(\"Running Validation...\")\n",
    "model.eval()\n",
    "\n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_attention_mask = batch[1].to(device)\n",
    "    b_emoji_indices = batch[2].to(device)\n",
    "    b_labels = batch[3].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, emoji_indices=b_emoji_indices)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(outputs, b_labels)\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    labels = b_labels.cpu().numpy()\n",
    "\n",
    "    # Convert one-hot encoded labels back to their integer form\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "\n",
    "    # Now compare preds and labels\n",
    "    total_eval_accuracy += np.sum(preds == labels) / len(labels)\n",
    "\n",
    "avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "print(f\"  Accuracy: {avg_val_accuracy:.2f}\")\n",
    "print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe9f232-d466-4f44-9f51-bf1da601b8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_save_path = \"bert_lstm_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef4e995-221c-48e5-9727-ed20f672eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\AppData\\Local\\Temp\\ipykernel_13872\\2469752451.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BERTLSTMModel:\n\tsize mismatch for emoji_embedding.weight: copying a param with shape torch.Size([223, 50]) from checkpoint, the shape in current model is torch.Size([3, 50]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Load the model's saved state\u001b[39;00m\n\u001b[0;32m     42\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_lstm_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Move the model to CPU/GPU\u001b[39;00m\n\u001b[0;32m     46\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BERTLSTMModel:\n\tsize mismatch for emoji_embedding.weight: copying a param with shape torch.Size([223, 50]) from checkpoint, the shape in current model is torch.Size([3, 50])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Dummy text and emoji data for testing\n",
    "dummy_texts = [\n",
    "    \"I love this!\",  # positive\n",
    "    \"This is the worst\",  # negative\n",
    "    \"I feel neutral about this\",  # neutral\n",
    "]\n",
    "\n",
    "dummy_emojis = [\"ðŸ˜€\", \"ðŸ˜¡\", \"ðŸ˜\"]  # Positive, Negative, Neutral emojis\n",
    "\n",
    "# Dummy labels (optional, only for comparison in this case)\n",
    "dummy_labels = [\n",
    "    [1, 0, 0],  # positive\n",
    "    [0, 1, 0],  # negative\n",
    "    [0, 0, 1],  # neutral\n",
    "]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dummy texts\n",
    "input_ids = [tokenizer.encode(text, add_special_tokens=True, max_length=64, truncation=True, padding='max_length') for text in dummy_texts]\n",
    "attention_masks = [[1 if token > 0 else 0 for token in tokens] for tokens in input_ids]\n",
    "\n",
    "# Emoji dictionary (same as used during training)\n",
    "emoji_dict = {\"ðŸ˜€\": 0, \"ðŸ˜¡\": 1, \"ðŸ˜\": 2}  # Example emoji to index mapping\n",
    "emoji_indices = [emoji_dict[emoji] for emoji in dummy_emojis]\n",
    "\n",
    "# Convert to tensors\n",
    "X_input_ids = torch.tensor(input_ids)\n",
    "X_attention_masks = torch.tensor(attention_masks)\n",
    "X_emoji_indices = torch.tensor(emoji_indices)\n",
    "y_labels = torch.tensor(dummy_labels)  # Optional: for comparison if you want\n",
    "\n",
    "# Load your model architecture (same as during training)\n",
    "model = BERTLSTMModel('bert-base-uncased', max_emojis=len(emoji_dict), embedding_size=50, lstm_hidden_size=50, num_labels=len(dummy_labels[0]))\n",
    "\n",
    "# Load the model's saved state\n",
    "model_save_path = \"bert_lstm_model.pth\"\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Move the model to CPU/GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Move the input tensors to the same device as the model\n",
    "X_input_ids = X_input_ids.to(device)\n",
    "X_attention_masks = X_attention_masks.to(device)\n",
    "X_emoji_indices = X_emoji_indices.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run the model on the dummy data\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_input_ids, attention_mask=X_attention_masks, emoji_indices=X_emoji_indices)\n",
    "\n",
    "# Get the predictions\n",
    "preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# (Optional) If comparing with true labels:\n",
    "true_labels = np.argmax(y_labels.cpu().numpy(), axis=1)\n",
    "\n",
    "# Print the results\n",
    "for i, text in enumerate(dummy_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Emoji: {dummy_emojis[i]}\")\n",
    "    print(f\"Predicted label: {preds[i]} (0=Positive, 1=Negative, 2=Neutral)\")\n",
    "    if dummy_labels:\n",
    "        print(f\"True label: {true_labels[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d92260d-6a90-411a-9540-9f9c336a7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\AppData\\Local\\Temp\\ipykernel_13872\\1537872983.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path), strict=False)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BERTLSTMModel:\n\tsize mismatch for emoji_embedding.weight: copying a param with shape torch.Size([223, 50]) from checkpoint, the shape in current model is torch.Size([3, 50]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Load the model's saved state with strict=False (ignores size mismatch)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_lstm_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Move the model to CPU/GPU\u001b[39;00m\n\u001b[0;32m     46\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BERTLSTMModel:\n\tsize mismatch for emoji_embedding.weight: copying a param with shape torch.Size([223, 50]) from checkpoint, the shape in current model is torch.Size([3, 50])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Dummy text and emoji data for testing\n",
    "dummy_texts = [\n",
    "    \"I love this!\",  # positive\n",
    "    \"This is the worst\",  # negative\n",
    "    \"I feel neutral about this\",  # neutral\n",
    "]\n",
    "\n",
    "dummy_emojis = [\"ðŸ˜€\", \"ðŸ˜¡\", \"ðŸ˜\"]  # Positive, Negative, Neutral emojis\n",
    "\n",
    "# Dummy labels (optional, only for comparison in this case)\n",
    "dummy_labels = [\n",
    "    [1, 0, 0],  # positive\n",
    "    [0, 1, 0],  # negative\n",
    "    [0, 0, 1],  # neutral\n",
    "]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dummy texts\n",
    "input_ids = [tokenizer.encode(text, add_special_tokens=True, max_length=64, truncation=True, padding='max_length') for text in dummy_texts]\n",
    "attention_masks = [[1 if token > 0 else 0 for token in tokens] for tokens in input_ids]\n",
    "\n",
    "# Emoji dictionary (same as used during training)\n",
    "emoji_dict = {\"ðŸ˜€\": 0, \"ðŸ˜¡\": 1, \"ðŸ˜\": 2}  # Example emoji to index mapping\n",
    "emoji_indices = [emoji_dict[emoji] for emoji in dummy_emojis]\n",
    "\n",
    "# Convert to tensors\n",
    "X_input_ids = torch.tensor(input_ids)\n",
    "X_attention_masks = torch.tensor(attention_masks)\n",
    "X_emoji_indices = torch.tensor(emoji_indices)\n",
    "y_labels = torch.tensor(dummy_labels)  # Optional: for comparison if you want\n",
    "\n",
    "# Load your model architecture with the current number of emojis (3 emojis)\n",
    "model = BERTLSTMModel('bert-base-uncased', max_emojis=3, embedding_size=50, lstm_hidden_size=50, num_labels=len(dummy_labels[0]))\n",
    "\n",
    "# Load the model's saved state with strict=False (ignores size mismatch)\n",
    "model_save_path = \"bert_lstm_model.pth\"\n",
    "model.load_state_dict(torch.load(model_save_path), strict=False)\n",
    "\n",
    "# Move the model to CPU/GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Move the input tensors to the same device as the model\n",
    "X_input_ids = X_input_ids.to(device)\n",
    "X_attention_masks = X_attention_masks.to(device)\n",
    "X_emoji_indices = X_emoji_indices.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run the model on the dummy data\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_input_ids, attention_mask=X_attention_masks, emoji_indices=X_emoji_indices)\n",
    "\n",
    "# Get the predictions\n",
    "preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# (Optional) If comparing with true labels:\n",
    "true_labels = np.argmax(y_labels.cpu().numpy(), axis=1)\n",
    "\n",
    "# Print the results\n",
    "for i, text in enumerate(dummy_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Emoji: {dummy_emojis[i]}\")\n",
    "    print(f\"Predicted label: {preds[i]} (0=Positive, 1=Negative, 2=Neutral)\")\n",
    "    if dummy_labels:\n",
    "        print(f\"True label: {true_labels[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc9e775-f3d7-49a4-bc2e-b599c26b7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, random_split\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f4a4e9a-6b30-41d0-89eb-b5994f2f5f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in salon reviews dataset:  Index(['salon_name', 'beautician ', 'Service_type', 'Review', 'Unnamed: 4',\n",
      "       'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your datasets (update with your actual file paths)\n",
    "salon_review_path = 'RAMI salon.xlsx'\n",
    "emoji_data_path = 'Emoji_Sentiment_Data_200.csv'\n",
    "\n",
    "# Load salon review data and emoji sentiment data\n",
    "salon_reviews = pd.read_excel(salon_review_path)\n",
    "emoji_data = pd.read_csv(emoji_data_path)\n",
    "\n",
    "# Check the column names of the salon_reviews dataframe\n",
    "print(\"Columns in salon reviews dataset: \", salon_reviews.columns)\n",
    "\n",
    "# Preprocessing for the salon reviews based on available columns\n",
    "# Dynamically select the columns if they exist\n",
    "expected_columns = ['salon_name', 'beautician', 'Service_type', 'Review']\n",
    "available_columns = [col for col in expected_columns if col in salon_reviews.columns]\n",
    "salon_reviews = salon_reviews[available_columns]\n",
    "\n",
    "# Preprocessing for emoji dataset\n",
    "emoji_data = emoji_data[['Emoji', 'Sentiment']]\n",
    "\n",
    "# Encode emoji sentiment\n",
    "emoji_encoder = LabelEncoder()\n",
    "emoji_data['Emoji_index'] = emoji_encoder.fit_transform(emoji_data['Emoji'])\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize reviews\n",
    "def preprocess_reviews(reviews):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for review in reviews:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Extract and preprocess reviews\n",
    "input_ids, attention_masks = preprocess_reviews(salon_reviews['Review'].values)\n",
    "\n",
    "# Map emojis in reviews to their sentiment index\n",
    "def extract_emojis_from_review(review, emoji_data):\n",
    "    emojis = [char for char in review if char in emoji_data['Emoji'].values]\n",
    "    if emojis:\n",
    "        return emoji_data[emoji_data['Emoji'] == emojis[0]]['Emoji_index'].values[0]\n",
    "    return emoji_encoder.transform(['ðŸ˜Š'])[0]  # Default emoji if none found\n",
    "\n",
    "salon_reviews['Emoji_index'] = salon_reviews['Review'].apply(lambda x: extract_emojis_from_review(x, emoji_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae07e49-29ff-460b-ad38-e0f69b2d8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emojis and labels to tensors\n",
    "emoji_indices = torch.tensor(salon_reviews['Emoji_index'].values)\n",
    "labels = torch.tensor([1 if 'positive' in review.lower() else 0 for review in salon_reviews['Review'].values])\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks, emoji_indices, labels)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler=RandomSampler(val_dataset), batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "619644dc-b87a-4a03-ab96-814bba132c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "class BERTLSTMWithSentiment(nn.Module):\n",
    "    def __init__(self, bert_model_name, max_emojis, embedding_size, lstm_hidden_size):\n",
    "        super(BERTLSTMWithSentiment, self).__init__()\n",
    "        \n",
    "        # BERT for text classification (sentiment)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.sentiment_model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.bert_to_lstm = nn.Linear(self.bert.config.hidden_size, lstm_hidden_size)\n",
    "        \n",
    "        self.emoji_embedding = nn.Embedding(num_embeddings=max_emojis, embedding_dim=embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        \n",
    "        self.text_weight = nn.Parameter(torch.tensor(0.85))  \n",
    "        self.emoji_weight = nn.Parameter(torch.tensor(0.15))  \n",
    "        \n",
    "        self.regressor = nn.Linear(lstm_hidden_size + 1, 1)  # +1 for sentiment score\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, emoji_indices):\n",
    "        # BERT for text embeddings\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled_output = bert_outputs.pooler_output\n",
    "        bert_pooled_output = self.dropout(bert_pooled_output)\n",
    "        bert_pooled_output = self.bert_to_lstm(bert_pooled_output)\n",
    "        \n",
    "        # BERT for sentiment\n",
    "        sentiment_outputs = self.sentiment_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentiment_score = torch.softmax(sentiment_outputs.logits, dim=1)[:, -1].unsqueeze(1)  # Take the negative sentiment score\n",
    "        \n",
    "        # Emoji embeddings and LSTM\n",
    "        emoji_embeds = self.emoji_embedding(emoji_indices)\n",
    "        lstm_out, _ = self.lstm(emoji_embeds)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Combine text, emoji, and sentiment score\n",
    "        combined_output = self.text_weight * bert_pooled_output + self.emoji_weight * lstm_out\n",
    "        combined_output = torch.cat((combined_output, sentiment_score), dim=1)  # Include sentiment score\n",
    "        \n",
    "        # Final prediction\n",
    "        score = self.regressor(combined_output)\n",
    "        score = self.sigmoid(score) * 5  # Scale to 0-5\n",
    "        \n",
    "        return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "embedding_size = 50\n",
    "lstm_hidden_size = 50\n",
    "num_labels = 2\n",
    "max_emojis = len(emoji_encoder.classes_)\n",
    "\n",
    "# Load the pre-trained sentiment classification model, ignoring size mismatch for the classifier\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'nlptown/bert-base-multilingual-uncased-sentiment', \n",
    "    num_labels=1,  # Modify for your regression task\n",
    "    ignore_mismatched_sizes=True  # Ignore size mismatch for the classifier\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc82d632-67af-457e-93b7-056655983b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 4.56\n",
      "Running Validation...\n",
      "  Validation Loss: 3.25\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 2.60\n",
      "Running Validation...\n",
      "  Validation Loss: 1.94\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 1.52\n",
      "Running Validation...\n",
      "  Validation Loss: 1.24\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 1.03\n",
      "Running Validation...\n",
      "  Validation Loss: 0.86\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.68\n",
      "Running Validation...\n",
      "  Validation Loss: 0.60\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.50\n",
      "Running Validation...\n",
      "  Validation Loss: 0.48\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.39\n",
      "Running Validation...\n",
      "  Validation Loss: 0.36\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.33\n",
      "Running Validation...\n",
      "  Validation Loss: 0.33\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.29\n",
      "Running Validation...\n",
      "  Validation Loss: 0.30\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.26\n",
      "Running Validation...\n",
      "  Validation Loss: 0.29\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "embedding_size = 50\n",
    "lstm_hidden_size = 50\n",
    "max_emojis = len(emoji_encoder.classes_)  # Assuming you have emoji encoder\n",
    "\n",
    "# Initialize the model with BERT and LSTM for emoji\n",
    "model = BERTLSTMModel('bert-base-uncased', max_emojis=max_emojis, embedding_size=embedding_size, lstm_hidden_size=lstm_hidden_size)\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Loss function for regression (MSELoss for continuous scores)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch_i in range(epochs):\n",
    "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    model.train()  # Set model to train mode\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_emoji_indices = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device).float()  # Ensure labels are float for regression\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, emoji_indices=b_emoji_indices)\n",
    "        \n",
    "        # Compute the loss (MSE for regression)\n",
    "        loss = loss_fn(outputs.squeeze(), b_labels)  # Squeeze to handle dimensions\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_emoji_indices = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask, emoji_indices=b_emoji_indices)\n",
    "\n",
    "        # Compute validation loss (MSE)\n",
    "        loss = loss_fn(outputs.squeeze(), b_labels)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5570a3dd-c976-473e-a0fd-ad79a71b62ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_lstm_model1.pth\n"
     ]
    }
   ],
   "source": [
    "# After training the model, save it\n",
    "model_save_path = \"bert_lstm_model1.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7ef5341-4ec4-4da7-b7ca-c1419c4200b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\AppData\\Local\\Temp\\ipykernel_8440\\3537068206.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model's state dictionary\n",
    "model_save_path = \"bert_lstm_model1.pth\"  # or the full path\n",
    "if os.path.exists(model_save_path):\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"File not found: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8577bf88-a661-4f66-bcb5-664054b65571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I had a fantastic haircut! Highly recommend this salon.\n",
      "Emoji: â¤ï¸\n",
      "Predicted Score: 0.51 / 5\n",
      "\n",
      "Review: Worst service ever. Would not go back.\n",
      "Emoji: ðŸ˜¡\n",
      "Predicted Score: 0.55 / 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of test reviews and emojis\n",
    "dummy_reviews = [\n",
    "    \"I had a fantastic haircut! Highly recommend this salon.\",  # Expected score: ~5\n",
    "    \"Worst service ever. Would not go back.\",  # Expected score: ~1\n",
    "]\n",
    "\n",
    "dummy_emojis = [\"â¤ï¸\", \"ðŸ˜¡\"]  # Positive and negative emojis\n",
    "\n",
    "# Function to preprocess the review and emoji before testing\n",
    "def preprocess_single_review(review, emoji, emoji_data):\n",
    "    # Tokenize the review using the BERT tokenizer\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # Convert the emoji to its corresponding index\n",
    "    if emoji in emoji_data['Emoji'].values:\n",
    "        emoji_index = emoji_data[emoji_data['Emoji'] == emoji]['Emoji_index'].values[0]\n",
    "    else:\n",
    "        emoji_index = emoji_encoder.transform(['ðŸ˜Š'])[0]  # Default emoji if not found\n",
    "\n",
    "    emoji_index = torch.tensor([[emoji_index]]).to(device)\n",
    "    \n",
    "    return input_ids, attention_mask, emoji_index\n",
    "\n",
    "# Testing the model with dummy reviews\n",
    "for review, emoji in zip(dummy_reviews, dummy_emojis):\n",
    "    input_ids, attention_mask, emoji_index = preprocess_single_review(review, emoji, emoji_data)\n",
    "    \n",
    "    # Forward pass through the model (in evaluation mode)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predicted_score = model(input_ids, attention_mask=attention_mask, emoji_indices=emoji_index)\n",
    "\n",
    "    # Output the predicted score\n",
    "    predicted_score = predicted_score.squeeze().item()  # Get the scalar value\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Emoji: {emoji}\")\n",
    "    print(f\"Predicted Score: {predicted_score:.2f} / 5\")  # The predicted score out of 5\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddc8eb0a-f2d9-4e4f-9cc4-f81de98a8ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I had a fantastic haircut! Highly recommend this salon.\n",
      "Emoji: â¤ï¸\n",
      "Predicted Score: 0.51 / 5\n",
      "\n",
      "Review: I had a fantastic haircut! Highly recommend this salon\n",
      "Emoji: ðŸ˜¡\n",
      "Predicted Score: 0.00 / 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of negative words and emojis for deduction\n",
    "negative_words = ['bad', 'worst', 'terrible', 'unprofessional', 'poor', 'horrible']\n",
    "negative_emojis = ['ðŸ˜¡', 'ðŸ˜”', 'ðŸ˜¢', 'ðŸ˜­', 'ðŸ˜ž', 'ðŸ˜ ']\n",
    "\n",
    "# Test the model with dummy reviews\n",
    "dummy_reviews = [\n",
    "    \"I had a fantastic haircut! Highly recommend this salon.\",  # Expected score: ~5\n",
    "    \"I had a fantastic haircut! Highly recommend this salon\",  # Expected score: ~1\n",
    "]\n",
    "\n",
    "dummy_emojis = [\"â¤ï¸\", \"ðŸ˜¡\"]  # Positive and negative emojis\n",
    "\n",
    "for review, emoji in zip(dummy_reviews, dummy_emojis):\n",
    "    input_ids, attention_mask, emoji_index = preprocess_single_review(review, emoji, emoji_data)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        predicted_score = model(input_ids, attention_mask=attention_mask, emoji_indices=emoji_index)\n",
    "    \n",
    "    # Get the predicted score\n",
    "    predicted_score = predicted_score.squeeze().item()\n",
    "\n",
    "    # Check for negative words in the review\n",
    "    for word in negative_words:\n",
    "        if word in review.lower():\n",
    "            predicted_score -= 1  # Deduct 1 point for negative words\n",
    "\n",
    "    # Check for negative emoji\n",
    "    if emoji in negative_emojis:\n",
    "        predicted_score -= 1  # Deduct 1 point for negative emojis\n",
    "\n",
    "    # Ensure the score stays between 0 and 5\n",
    "    predicted_score = max(0, min(predicted_score, 5))\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Emoji: {emoji}\")\n",
    "    print(f\"Predicted Score: {predicted_score:.2f} / 5\")  # Final adjusted score\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23a73ea6-6b11-4144-8a34-df8eaa30acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I had a fantastic haircut! Highly recommend this salon.\n",
      "Emoji: â¤ï¸\n",
      "Predicted Score: 2.48 / 5\n",
      "\n",
      "Review: Worst service ever. Would not go back.\n",
      "Emoji: ðŸ˜¡\n",
      "Predicted Score: 1.58 / 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of negative words and emojis for penalty\n",
    "negative_words = ['worst', 'bad', 'terrible', 'unprofessional', 'poor', 'horrible']\n",
    "negative_emojis = ['ðŸ˜¡', 'ðŸ˜”', 'ðŸ˜¢', 'ðŸ˜­', 'ðŸ˜ž', 'ðŸ˜ ']\n",
    "\n",
    "# Test the model with reviews and emojis\n",
    "dummy_reviews = [\n",
    "    \"I had a fantastic haircut! Highly recommend this salon.\",  # Expected score: High\n",
    "    \"Worst service ever. Would not go back.\",  # Expected score: Low\n",
    "]\n",
    "\n",
    "dummy_emojis = [\"â¤ï¸\", \"ðŸ˜¡\"]  # Positive and negative emojis\n",
    "\n",
    "for review, emoji in zip(dummy_reviews, dummy_emojis):\n",
    "    input_ids, attention_mask, emoji_index = preprocess_single_review(review, emoji, emoji_data)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        predicted_score = model(input_ids, attention_mask=attention_mask, emoji_indices=emoji_index)\n",
    "    \n",
    "    # Get the predicted score\n",
    "    predicted_score = predicted_score.squeeze().item()\n",
    "    \n",
    "    # Apply penalty for negative words and emojis\n",
    "    for word in negative_words:\n",
    "        if word in review.lower():\n",
    "            predicted_score -= 0.5  # Deduct a fixed amount for negative words\n",
    "    \n",
    "    if emoji in negative_emojis:\n",
    "        predicted_score -= 0.5  # Deduct for negative emoji\n",
    "    \n",
    "    # Ensure score stays between 0 and 5\n",
    "    predicted_score = max(0, min(predicted_score, 5))\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Emoji: {emoji}\")\n",
    "    print(f\"Predicted Score: {predicted_score:.2f} / 5\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "16cf8638-b199-43ca-bea2-4e9de47d3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I had a fantastic haircut! Highly recommend this salon.\n",
      "Emoji: â¤ï¸\n",
      "Predicted Score: 0.09 / 5\n",
      "\n",
      "Review: Worst service ever. Would not go back.\n",
      "Emoji: ðŸ˜¡\n",
      "Predicted Score: 0.09 / 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model with dummy reviews\n",
    "dummy_reviews = [\n",
    "    \"I had a fantastic haircut! Highly recommend this salon.\",  # Expected score: high\n",
    "    \"Worst service ever. Would not go back.\",  # Expected score: low\n",
    "]\n",
    "\n",
    "dummy_emojis = [\"â¤ï¸\", \"ðŸ˜¡\"]  # Positive and negative emojis\n",
    "\n",
    "for review, emoji in zip(dummy_reviews, dummy_emojis):\n",
    "    input_ids, attention_mask, emoji_index = preprocess_single_review(review, emoji, emoji_data)\n",
    "    \n",
    "    # Forward pass through the model (in evaluation mode)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predicted_score = model(input_ids, attention_mask=attention_mask, emoji_indices=emoji_index)\n",
    "\n",
    "    # Output the predicted score\n",
    "    predicted_score = predicted_score.squeeze().item()  # Get the scalar value\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Emoji: {emoji}\")\n",
    "    print(f\"Predicted Score: {predicted_score:.2f} / 5\")  # The predicted score out of 5\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431875ed-7aa7-4907-b3c0-7536bc948d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
